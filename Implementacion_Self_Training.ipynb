{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from joblib import load\n",
    "from keras.models import load_model, clone_model\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score\n",
    "\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "#sys.path.append('C:/Users/JoséEmmanuelParedesR/OneDrive - Exitus Credit/Documentos/SCRIPTS/Scripts_Python')\n",
    "from Funciones_tablas_validacion import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 31 variables\n",
      "S2 35 variables\n",
      "S3 36 variables\n",
      "S4 28 variables\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "import os\n",
    "os.chdir(\"C:/Users/RobertoRosasGuerrero/Analítica Luisito Valdez/Self Learning/202220809 SELF LEARNING_Nacho/Prueba_Self_Learning/Score_iBillete_Sol20\")\n",
    "#obtener lista de variables por segmento\n",
    "seg_lst = ['S1','S2','S3','S4']\n",
    "variables = {}\n",
    "for seg in seg_lst:\n",
    "    df_var = pd.read_excel('Variables.xlsx',sheet_name=seg)\n",
    "    variables = {**variables,**{seg: list(df_var['Variables'].values)}}\n",
    "    print(seg, len(variables[seg]), 'variables')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S1': ['AYC_011_Sum_woe',\n",
       "  'nombreOtorgante_12_AYC_026_Max_woe',\n",
       "  'tipoContrato_4_AYC_026_Mean_woe',\n",
       "  'tipoCuenta_3_END_002_Max_woe',\n",
       "  'EXP_004_Mean_woe',\n",
       "  'RES_016_BUR_woe',\n",
       "  'nombreOtorgante_10_SDO_001_BUR_Max_woe',\n",
       "  'tipoCuenta_3_SDO_002_BUR_Min_woe',\n",
       "  'INT_001_MOR_Sum_woe',\n",
       "  'nombreOtorgante_12_INT_017_MOR_Sum_woe',\n",
       "  'nombreOtorgante_5_INT_049_MOR_Sum_woe',\n",
       "  'nombreOtorgante_8_INT_034_MOR_Mean_woe',\n",
       "  'nombreOtorgante_8_INT_065_MOR_Mean_woe',\n",
       "  'nombreOtorgante_8_INT_097_MOR_Mean_woe',\n",
       "  'nombreOtorgante_10_INT_053_MOR_Min_woe',\n",
       "  'nombreOtorgante_10_INT_052_MOR_Max_woe',\n",
       "  'nombreOtorgante_8_INT_055_MOR_Max_woe',\n",
       "  'tipoCuenta_1_INT_026_MOR_Sum_woe',\n",
       "  'tipoCuenta_1_INT_060_MOR_Sum_woe',\n",
       "  'tipoCuenta_2_INT_086_MOR_Mean_woe',\n",
       "  'tipoCuenta_1_INT_053_MOR_Min_woe',\n",
       "  'tipoContrato_1_INT_006_MOR_Sum_woe',\n",
       "  'tipoContrato_4_INT_063_MOR_Sum_woe',\n",
       "  'tipoContrato_1_INT_010_MOR_Mean_woe',\n",
       "  'tipoContrato_4_INT_015_MOR_Mean_woe',\n",
       "  'tipoContrato_4_INT_057_MOR_Mean_woe',\n",
       "  'tipoContrato_1_INT_061_MOR_Mean_woe',\n",
       "  'tipoContrato_5_INT_114_MOR_Mean_woe',\n",
       "  'tipoContrato_1_INT_054_MOR_Min_woe',\n",
       "  'tipoContrato_1_INT_054_MOR_Max_woe',\n",
       "  'tipoContrato_5_INT_061_MOR_Max_woe'],\n",
       " 'S2': ['AYC_004_Sum_woe',\n",
       "  'AYC_024_Mean_woe',\n",
       "  'AYC_030_Max_woe',\n",
       "  'nombreOtorgante_8_AYC_023_Mean_woe',\n",
       "  'nombreOtorgante_3_AYC_018_Min_woe',\n",
       "  'nombreOtorgante_10_AYC_008_Max_woe',\n",
       "  'nombreOtorgante_10_AYC_023_Max_woe',\n",
       "  'tipoContrato_1_AYC_019_Mean_woe',\n",
       "  'tipoContrato_1_AYC_028_Mean_woe',\n",
       "  'tipoContrato_1_END_001_Sum_woe',\n",
       "  'tipoContrato_1_END_004_Mean_woe',\n",
       "  'nombreOtorgante_8_SDO_006_BUR_Max_woe',\n",
       "  'tipoCuenta_3_SDO_002_BUR_Max_woe',\n",
       "  'tipoContrato_4_SDO_002_BUR_Sum_woe',\n",
       "  'tipoContrato_4_SDO_006_BUR_Sum_woe',\n",
       "  'INT_002_MOR_Sum_woe',\n",
       "  'INT_005_MOR_Mean_woe',\n",
       "  'INT_011_MOR_Mean_woe',\n",
       "  'INT_039_MOR_Mean_woe',\n",
       "  'INT_045_MOR_Mean_woe',\n",
       "  'INT_069_MOR_Mean_woe',\n",
       "  'INT_032_MOR_Max_woe',\n",
       "  'nombreOtorgante_1_INT_006_MOR_Mean_woe',\n",
       "  'nombreOtorgante_8_INT_124_MOR_Mean_woe',\n",
       "  'nombreOtorgante_12_INT_050_MOR_Min_woe',\n",
       "  'nombreOtorgante_12_INT_065_MOR_Max_woe',\n",
       "  'nombreOtorgante_5_INT_067_MOR_Max_woe',\n",
       "  'tipoCuenta_1_INT_009_MOR_Mean_woe',\n",
       "  'tipoCuenta_1_INT_021_MOR_Mean_woe',\n",
       "  'tipoCuenta_1_INT_060_MOR_Mean_woe',\n",
       "  'tipoCuenta_3_INT_056_MOR_Min_woe',\n",
       "  'tipoContrato_5_INT_068_MOR_Mean_woe',\n",
       "  'tipoContrato_1_INT_086_MOR_Mean_woe',\n",
       "  'tipoContrato_2_INT_003_MOR_Max_woe',\n",
       "  'tipoContrato_4_INT_055_MOR_Max_woe'],\n",
       " 'S3': ['AYC_001_Mean_woe',\n",
       "  'nombreOtorgante_8_AYC_021_Max_woe',\n",
       "  'tipoCuenta_1_AYC_026_Max_woe',\n",
       "  'tipoContrato_4_AYC_009_Mean_woe',\n",
       "  'END_004_Mean_woe',\n",
       "  'nombreOtorgante_8_END_001_Mean_woe',\n",
       "  'tipoCuenta_1_END_004_Sum_woe',\n",
       "  'tipoCuenta_3_EXP_001_Min_woe',\n",
       "  'tipoContrato_5_EXP_001_Min_woe',\n",
       "  'RES_029_BUR_woe',\n",
       "  'nombreOtorgante_5_SDO_001_BUR_Sum_woe',\n",
       "  'nombreOtorgante_8_SDO_002_BUR_Sum_woe',\n",
       "  'tipoCuenta_2_SDO_002_BUR_Sum_woe',\n",
       "  'tipoCuenta_3_SDO_006_BUR_Min_woe',\n",
       "  'INT_002_MOR_Sum_woe',\n",
       "  'nombreOtorgante_12_INT_048_MOR_Sum_woe',\n",
       "  'nombreOtorgante_8_INT_047_MOR_Mean_woe',\n",
       "  'nombreOtorgante_1_INT_049_MOR_Mean_woe',\n",
       "  'nombreOtorgante_8_INT_139_MOR_Max_woe',\n",
       "  'tipoCuenta_2_INT_004_MOR_Mean_woe',\n",
       "  'tipoCuenta_1_INT_008_MOR_Mean_woe',\n",
       "  'tipoCuenta_1_INT_048_MOR_Mean_woe',\n",
       "  'tipoCuenta_1_INT_084_MOR_Mean_woe',\n",
       "  'tipoCuenta_2_INT_085_MOR_Min_woe',\n",
       "  'tipoCuenta_2_INT_038_MOR_Max_woe',\n",
       "  'tipoCuenta_2_INT_058_MOR_Max_woe',\n",
       "  'tipoContrato_1_INT_009_MOR_Mean_woe',\n",
       "  'tipoContrato_1_INT_038_MOR_Mean_woe',\n",
       "  'tipoContrato_1_INT_058_MOR_Mean_woe',\n",
       "  'tipoContrato_1_INT_063_MOR_Mean_woe',\n",
       "  'tipoContrato_4_INT_083_MOR_Mean_woe',\n",
       "  'tipoContrato_5_INT_087_MOR_Mean_woe',\n",
       "  'tipoContrato_1_INT_132_MOR_Mean_woe',\n",
       "  'tipoContrato_1_INT_013_MOR_Max_woe',\n",
       "  'tipoContrato_4_INT_050_MOR_Max_woe',\n",
       "  'tipoContrato_4_INT_070_MOR_Max_woe'],\n",
       " 'S4': ['AYC_020_Mean_woe',\n",
       "  'nombreOtorgante_13_AYC_009_Sum_woe',\n",
       "  'nombreOtorgante_10_AYC_001_Mean_woe',\n",
       "  'nombreOtorgante_10_AYC_011_Mean_woe',\n",
       "  'nombreOtorgante_10_AYC_029_Max_woe',\n",
       "  'tipoCuenta_3_AYC_030_Sum_woe',\n",
       "  'tipoContrato_1_AYC_017_Min_woe',\n",
       "  'tipoContrato_1_AYC_027_Min_woe',\n",
       "  'tipoContrato_1_AYC_029_Min_woe',\n",
       "  'tipoContrato_1_END_004_Min_woe',\n",
       "  'INT_001_MOR_Sum_woe',\n",
       "  'INT_022_MOR_Mean_woe',\n",
       "  'INT_073_MOR_Mean_woe',\n",
       "  'INT_090_MOR_Mean_woe',\n",
       "  'INT_144_MOR_Mean_woe',\n",
       "  'nombreOtorgante_12_INT_050_MOR_Sum_woe',\n",
       "  'nombreOtorgante_3_INT_148_MOR_Sum_woe',\n",
       "  'nombreOtorgante_1_INT_042_MOR_Mean_woe',\n",
       "  'nombreOtorgante_12_INT_080_MOR_Mean_woe',\n",
       "  'nombreOtorgante_8_INT_088_MOR_Mean_woe',\n",
       "  'nombreOtorgante_8_INT_120_MOR_Mean_woe',\n",
       "  'tipoCuenta_1_INT_152_MOR_Sum_woe',\n",
       "  'tipoCuenta_2_INT_062_MOR_Mean_woe',\n",
       "  'tipoCuenta_1_INT_105_MOR_Mean_woe',\n",
       "  'tipoCuenta_3_INT_111_MOR_Max_woe',\n",
       "  'tipoContrato_1_INT_101_MOR_Mean_woe',\n",
       "  'tipoContrato_1_INT_151_MOR_Mean_woe',\n",
       "  'tipoContrato_5_INT_107_MOR_Max_woe']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option(\"display.Max_rows\",None)\n",
    "{**variables,**{seg: list(df_var['Variables'].values)}}\n",
    "#{**variables,**{seg: list(df_var['Variables'].values)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se cargan los modelos actuales #Score_iBillete_Sol20/\n",
    "os.chdir(\"C:/Users/RobertoRosasGuerrero/Analítica Luisito Valdez/Self Learning\")\n",
    "model_S1 = load('Modelos/MLP_S1.pkl')\n",
    "model_S2 = load_model('Modelos/RNN_S2.h5')\n",
    "model_S3 = load('Modelos/MLP_S3.pkl')\n",
    "model_S4 = load('Modelos/MLP_S4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir variable target\n",
    "target = 'BGI'\n",
    "\n",
    "#cargar bases de entrenamiento, etiquetadas y sin etiquetar\n",
    "# S1\n",
    "df_e_S1 = pd.read_csv('Bases_seg_Trainning/Base_Entrenamiento_S1.csv')\n",
    "df_se_S1 = pd.read_csv('Bases_seg_Trainning/Base_Entrenamiento_sinEtiqueta_S1.csv')\n",
    "df_se_S1[target] = -1 #indicacion de no etiquetado\n",
    "# S2\n",
    "df_e_S2 = pd.read_csv('Bases_seg_Trainning/Base_Entrenamiento_S2.csv')\n",
    "df_se_S2 = pd.read_csv('Bases_seg_Trainning/Base_Entrenamiento_sinEtiqueta_S2.csv')\n",
    "df_se_S2[target] = -1\n",
    "# S3\n",
    "df_e_S3 = pd.read_csv('Bases_seg_Trainning/Base_Entrenamiento_S3.csv')\n",
    "df_se_S3 = pd.read_csv('Bases_seg_Trainning/Base_Entrenamiento_sinEtiqueta_S3.csv')\n",
    "df_se_S3[target] = -1\n",
    "# S4\n",
    "df_e_S4 = pd.read_csv('Bases_seg_Trainning/Base_Entrenamiento_S4.csv')\n",
    "df_se_S4 = pd.read_csv('Bases_seg_Trainning/Base_Entrenamiento_sinEtiqueta_S4.csv')\n",
    "df_se_S4[target] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar base de comprobación (test)\n",
    "#S1\n",
    "df_t_S1 = pd.read_csv('Bases_seg_Trainning/Base_Comprobacion_S1.csv')\n",
    "#S2\n",
    "df_t_S2 = pd.read_csv('Bases_seg_Trainning/Base_Comprobacion_S2.csv')\n",
    "#S3\n",
    "df_t_S3 = pd.read_csv('Bases_seg_Trainning/Base_Comprobacion_S3.csv')\n",
    "#S4\n",
    "df_t_S4 = pd.read_csv('Bases_seg_Trainning/Base_Comprobacion_S4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir el conjunto de train, test y sin etiquetar\n",
    "#S1\n",
    "X_train_S1, y_train_S1 = df_e_S1[variables['S1']], df_e_S1[target]\n",
    "X_se_S1, y_se_S1 = df_se_S1[variables['S1']], df_se_S1[target]\n",
    "X_test_S1, y_test_S1 = df_t_S1[variables['S1']], df_t_S1[target]\n",
    "#S2\n",
    "X_train_S2, y_train_S2 = df_e_S2[variables['S2']], df_e_S2[target]\n",
    "X_se_S2, y_se_S2 = df_se_S2[variables['S2']], df_se_S2[target]\n",
    "X_test_S2, y_test_S2 = df_t_S2[variables['S2']], df_t_S2[target]\n",
    "#S3\n",
    "X_train_S3, y_train_S3 = df_e_S3[variables['S3']], df_e_S3[target]\n",
    "X_se_S3, y_se_S3 = df_se_S3[variables['S3']], df_se_S3[target]\n",
    "X_test_S3, y_test_S3 = df_t_S3[variables['S3']], df_t_S3[target]\n",
    "#S4\n",
    "X_train_S4, y_train_S4 = df_e_S4[variables['S4']], df_e_S4[target]\n",
    "X_se_S4, y_se_S4 = df_se_S4[variables['S4']], df_se_S4[target]\n",
    "X_test_S4, y_test_S4 = df_t_S4[variables['S4']], df_t_S4[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbral = 0.7\n",
    "# scores = [] #[f1_score(y_test_S2,model_S2.predict(X_test_S2))]\n",
    "# num_sin_etiqueta = len(y_se_S2)\n",
    "# pseudo_etiquetas = [1]\n",
    "# iteracion = 1\n",
    "\n",
    "# while pseudo_etiquetas[-1] > 0:\n",
    "#     if iteracion == 1:\n",
    "#         pseudo_etiquetas.pop(0)\n",
    "        \n",
    "#     #calcular el score del modelo\n",
    "#     score = f1_score(y_test_S2, predict_clases(model_S2,X_test_S2))\n",
    "#     scores.append(score)\n",
    "#     #calcular la probabilidad de las no etiquetadas\n",
    "#     y_proba_S2 = model_S2.predict(X_se_S2)\n",
    "#     #obtener la etiqueta (clase) en base a la probabilidad, si ambas probas están debajo del umbral, se asigna -1\n",
    "#     y_etiquedado_S2 = pd.Series(retornar_clase_y(y_proba_S2, umbral=umbral))\n",
    "#     #determinar los indices de las que se etiquetaron y se cuentan\n",
    "#     index_etiquetados = y_etiquedado_S2[y_etiquedado_S2.isin([0,1])].index\n",
    "#     pseudo_etiquetas.append(len(index_etiquetados)) #contar cuantas se etiquetaron\n",
    "#     print(f'Iteración #{iteracion}\\t Etiquetados: {pseudo_etiquetas[-1]}. Etiquetado total: {sum(pseudo_etiquetas)}/{num_sin_etiqueta}')\n",
    "#     #agregar los etiquetados al conjunto de entrenamiento\n",
    "#     X_train_S2 = pd.concat([X_train_S2, X_se_S2.loc[index_etiquetados]]).reset_index(drop=True)\n",
    "#     y_train_S2 = pd.concat([y_train_S2,y_etiquedado_S2.loc[index_etiquetados]]).reset_index(drop=True)\n",
    "#     #eliminar los etiquetados del conjunto de no etiquetados\n",
    "#     X_se_S2 = X_se_S2.drop(index=index_etiquetados).reset_index(drop=True)\n",
    "#     y_se_S2 = y_se_S2.drop(index=index_etiquetados).reset_index(drop=True)\n",
    "#     #se reentrena\n",
    "#     model_S2.fit(X_train_S2, y_train_S2)\n",
    "#     #agregar al conteo de la iteracion\n",
    "#     iteracion += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reentrenar_modelo(modelo, c_train, c_test, c_se, umbral, n_iter = None):\n",
    "    \n",
    "    #hacer una copia profunda de las variables para que no se modifiquen las originales\n",
    "    model = deepcopy(modelo)\n",
    "    X_train = c_train[0].copy()\n",
    "    y_train = c_train[1].copy()\n",
    "    X_test = c_test[0].copy()\n",
    "    y_test = c_test[1].copy()\n",
    "    X_se = c_se[0].copy()\n",
    "    y_se = c_se[1].copy()\n",
    "    \n",
    "    #bandera para saber el valor del umbral\n",
    "    print('Umbral: ', umbral)\n",
    "    #comienza el proceso de reentrenamiento\n",
    "    scores = [] #lista para guardar el score\n",
    "    num_sin_etiqueta = len(y_se)\n",
    "    pseudo_etiquetas = [1]\n",
    "    iteracion = 1\n",
    "\n",
    "    while pseudo_etiquetas[-1] > 0:\n",
    "        if iteracion == 1:\n",
    "            pseudo_etiquetas.pop(0)\n",
    "            \n",
    "        #calcular el score del modelo\n",
    "        score = f1_score(y_test, model.predict(X_test))\n",
    "        scores.append(score)\n",
    "        #calcular la probabilidad de las no etiquetadas\n",
    "        y_proba = model.predict_proba(X_se)\n",
    "        #obtener la etiqueta (clase) en base a la probabilidad, si ambas probas están debajo del umbral, se asigna -1\n",
    "        y_etiquedado = pd.Series(retornar_clase_y(y_proba, umbral))\n",
    "        #determinar los indices de las que se etiquetaron y se cuentan\n",
    "        index_etiquetados = y_etiquedado[y_etiquedado.isin([0,1])].index\n",
    "        pseudo_etiquetas.append(len(index_etiquetados)) #contar cuantas se etiquetaron\n",
    "        print(f'Iteración #{iteracion}\\t Etiquetados: {pseudo_etiquetas[-1]}. Etiquetado total: {sum(pseudo_etiquetas)}/{num_sin_etiqueta}')\n",
    "        #agregar los etiquetados al conjunto de entrenamiento\n",
    "        X_train = pd.concat([X_train, X_se.loc[index_etiquetados]]).reset_index(drop=True)\n",
    "        y_train = pd.concat([y_train,y_etiquedado.loc[index_etiquetados]]).reset_index(drop=True)\n",
    "        #eliminar los etiquetados del conjunto de no etiquetados\n",
    "        X_se = X_se.drop(index=index_etiquetados).reset_index(drop=True)\n",
    "        y_se = y_se.drop(index=index_etiquetados).reset_index(drop=True)\n",
    "        #se reentrena\n",
    "        model.fit(X_train, y_train)\n",
    "        #agregar al conteo de la iteracion\n",
    "        iteracion += 1\n",
    "        #cortar el proceso si en la primera vuelta se etiqueta a todos\n",
    "        if X_se.shape[0] == 0:\n",
    "            break\n",
    "        #cortar por número de iteraciones\n",
    "        if n_iter == None:\n",
    "            pass\n",
    "        else:\n",
    "            if iteracion == n_iter + 1:\n",
    "                break\n",
    "        \n",
    "    return model, scores, pseudo_etiquetas\n",
    "\n",
    "def reentrenar_modelo_RN(modelo, c_train, c_test, c_se, umbral, n_iter=None):\n",
    "    \n",
    "    #hacer una copia profunda de las variables para que no se modifiquen las originales\n",
    "    model = clone_model(modelo)\n",
    "    X_train = c_train[0].copy()\n",
    "    y_train = c_train[1].copy()\n",
    "    X_test = c_test[0].copy()\n",
    "    y_test = c_test[1].copy()\n",
    "    X_se = c_se[0].copy()\n",
    "    y_se = c_se[1].copy()\n",
    "    \n",
    "    #bandera para saber el valor del umbral\n",
    "    print('Umbral: ', f'{umbral:.2f}')\n",
    "    #comienza el proceso de reentrenamiento\n",
    "    scores = [] #lista para guardar el score\n",
    "    num_sin_etiqueta = len(y_se)\n",
    "    pseudo_etiquetas = [1]\n",
    "    iteracion = 1\n",
    "\n",
    "    while pseudo_etiquetas[-1] > 0:\n",
    "        if iteracion == 1:\n",
    "            pseudo_etiquetas.pop(0)\n",
    "            \n",
    "        #calcular el score del modelo\n",
    "        score = f1_score(y_test, predict_clases(model,X_test))\n",
    "        scores.append(score)\n",
    "        #calcular la probabilidad de las no etiquetadas\n",
    "        y_proba = model.predict(X_se)\n",
    "        #obtener la etiqueta (clase) en base a la probabilidad, si ambas probas están debajo del umbral, se asigna -1\n",
    "        y_etiquedado = pd.Series(retornar_clase_y(y_proba, umbral))\n",
    "        #determinar los indices de las que se etiquetaron y se cuentan\n",
    "        index_etiquetados = y_etiquedado[y_etiquedado.isin([0,1])].index\n",
    "        pseudo_etiquetas.append(len(index_etiquetados)) #contar cuantas se etiquetaron\n",
    "        print(f'Iteración #{iteracion}\\t Etiquetados: {pseudo_etiquetas[-1]}. Etiquetado total: {sum(pseudo_etiquetas)}/{num_sin_etiqueta}')\n",
    "        #agregar los etiquetados al conjunto de entrenamiento\n",
    "        X_train = pd.concat([X_train, X_se.loc[index_etiquetados]]).reset_index(drop=True)\n",
    "        y_train = pd.concat([y_train,y_etiquedado.loc[index_etiquetados]]).reset_index(drop=True)\n",
    "        #eliminar los etiquetados del conjunto de no etiquetados\n",
    "        X_se = X_se.drop(index=index_etiquetados).reset_index(drop=True)\n",
    "        y_se = y_se.drop(index=index_etiquetados).reset_index(drop=True)\n",
    "        #compilar el modelo\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #se reentrena\n",
    "        model.fit(X_train, y_train, verbose=0)\n",
    "        #agregar al conteo de la iteracion\n",
    "        iteracion += 1\n",
    "        #cortar el proceso si en la primera vuelta se etiqueta a todos\n",
    "        if X_se.shape[0] == 0:\n",
    "            break\n",
    "        #cortar por número de iteraciones\n",
    "        if n_iter == None:\n",
    "            pass\n",
    "        else:\n",
    "            if iteracion == n_iter + 1:\n",
    "                break\n",
    "        \n",
    "    return model, scores, pseudo_etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinar los mejores valores de umbral y n_iteraciones para cada segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbral_valores = np.arange(0.50, 1.05, 0.05)\n",
    "umbral_valores = np.append(umbral_valores, 0.99)\n",
    "umbral_valores = sorted(umbral_valores)\n",
    "umbral_valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umbral = 0.7\n",
    "# scores = [] #[f1_score(y_test_S1,model_S1.predict(X_test_S1))]\n",
    "# num_sin_etiqueta = len(y_se_S1)\n",
    "# pseudo_etiquetas = [1]\n",
    "# iteracion = 1\n",
    "\n",
    "# while pseudo_etiquetas[-1] > 0:\n",
    "#     if iteracion == 1:\n",
    "#         pseudo_etiquetas.pop(0)\n",
    "        \n",
    "#     #calcular el score del modelo\n",
    "#     score = f1_score(y_test_S1, model_S1.predict(X_test_S1))\n",
    "#     scores.append(score)\n",
    "#     #calcular la probabilidad de las no etiquetadas\n",
    "#     y_proba_S1 = model_S1.predict_proba(X_se_S1)\n",
    "#     #obtener la etiqueta (clase) en base a la probabilidad, si ambas probas están debajo del umbral, se asigna -1\n",
    "#     y_etiquedado_S1 = pd.Series(retornar_clase_y(y_proba_S1, umbral=umbral))\n",
    "#     #determinar los indices de las que se etiquetaron y se cuentan\n",
    "#     index_etiquetados = y_etiquedado_S1[y_etiquedado_S1.isin(model_S1.classes_)].index\n",
    "#     pseudo_etiquetas.append(len(index_etiquetados)) #contar cuantas se etiquetaron\n",
    "#     print(f'Iteración #{iteracion}\\t Etiquetados: {pseudo_etiquetas[-1]}. Etiquetado total: {sum(pseudo_etiquetas)}/{num_sin_etiqueta}')\n",
    "#     #agregar los etiquetados al conjunto de entrenamiento\n",
    "#     X_train_S1 = pd.concat([X_train_S1, X_se_S1.loc[index_etiquetados]]).reset_index(drop=True)\n",
    "#     y_train_S1 = pd.concat([y_train_S1,y_etiquedado_S1.loc[index_etiquetados]]).reset_index(drop=True)\n",
    "#     #eliminar los etiquetados del conjunto de no etiquetados\n",
    "#     X_se_S1 = X_se_S1.drop(index=index_etiquetados).reset_index(drop=True)\n",
    "#     y_se_S1 = y_se_S1.drop(index=index_etiquetados).reset_index(drop=True)\n",
    "#     #se reentrena\n",
    "#     model_S1.fit(X_train_S1, y_train_S1)\n",
    "#     #agregar al conteo de la iteracion\n",
    "#     iteracion += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbrales = []\n",
    "scores_umbral = []\n",
    "pseudo_etiquetas_umbral = []\n",
    "   \n",
    "for umbral in umbral_valores:\n",
    "    umbrales.append(umbral)\n",
    "    scores, pseudo_etiquetas = reentrenar_modelo(model_S1, (X_train_S1, y_train_S1), (X_test_S1,y_test_S1), (X_se_S1, y_se_S1), umbral=umbral)[1:]\n",
    "    scores_umbral.append(scores)\n",
    "    pseudo_etiquetas_umbral.append(pseudo_etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,7))\n",
    "plt.subplot(2,1,1)\n",
    "for umbral, scores in zip(umbrales, scores_umbral):\n",
    "    plt.plot(range(len(scores)), scores, label = f'{umbral:.2f}')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('f1 score')\n",
    "plt.title('Performance autoentrenamiento modelo S1')\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for umbral, pseudo_etiquetas in zip(umbrales,pseudo_etiquetas_umbral):\n",
    "    pseudo_etiquetas_total = [sum(pseudo_etiquetas[:i]) for i in range(1,len(pseudo_etiquetas)+1)]\n",
    "    plt.plot(range(len(pseudo_etiquetas_total)), pseudo_etiquetas_total, label = f'{umbral:.2f}')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Total etiquetados')\n",
    "#plt.title('Etiquetado autoentrenamiento modelo S1')\n",
    "#plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbrales = []\n",
    "scores_umbral = []\n",
    "pseudo_etiquetas_umbral = []\n",
    "   \n",
    "for umbral in umbral_valores:\n",
    "    umbrales.append(umbral)\n",
    "    scores, pseudo_etiquetas = reentrenar_modelo_RN(model_S2, (X_train_S2, y_train_S2), (X_test_S2,y_test_S2), (X_se_S2, y_se_S2), umbral=umbral)[1:]\n",
    "    scores_umbral.append(scores)\n",
    "    pseudo_etiquetas_umbral.append(pseudo_etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(10,7))\n",
    "plt.subplot(2,1,1)\n",
    "for umbral, scores in zip(umbrales, scores_umbral):\n",
    "    plt.plot(range(len(scores)), scores, label = f'{umbral:.2f}')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('f1 score')\n",
    "plt.title('Performance autoentrenamiento modelo S2')\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for umbral, pseudo_etiquetas in zip(umbrales,pseudo_etiquetas_umbral):\n",
    "    pseudo_etiquetas_total = [sum(pseudo_etiquetas[:i]) for i in range(1,len(pseudo_etiquetas)+1)]\n",
    "    plt.plot(range(len(pseudo_etiquetas_total)), pseudo_etiquetas_total, label = f'{umbral:.2f}')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Total etiquetados')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbrales = []\n",
    "scores_umbral = []\n",
    "pseudo_etiquetas_umbral = []\n",
    "   \n",
    "for umbral in umbral_valores:\n",
    "    umbrales.append(umbral)\n",
    "    scores, pseudo_etiquetas = reentrenar_modelo(model_S3, (X_train_S3, y_train_S3), (X_test_S3,y_test_S3), (X_se_S3, y_se_S3), umbral=umbral)[1:]\n",
    "    scores_umbral.append(scores)\n",
    "    pseudo_etiquetas_umbral.append(pseudo_etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(10,7))\n",
    "plt.subplot(2,1,1)\n",
    "for umbral, scores in zip(umbrales, scores_umbral):\n",
    "    plt.plot(range(len(scores)), scores, label = f'{umbral:.2f}')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('f1 score')\n",
    "plt.title('Performance autoentrenamiento modelo S3')\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for umbral, pseudo_etiquetas in zip(umbrales,pseudo_etiquetas_umbral):\n",
    "    pseudo_etiquetas_total = [sum(pseudo_etiquetas[:i]) for i in range(1,len(pseudo_etiquetas)+1)]\n",
    "    plt.plot(range(len(pseudo_etiquetas_total)), pseudo_etiquetas_total, label = f'{umbral:.2f}')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Total etiquetados')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbrales = []\n",
    "scores_umbral = []\n",
    "pseudo_etiquetas_umbral = []\n",
    "   \n",
    "for umbral in umbral_valores:\n",
    "    umbrales.append(umbral)\n",
    "    scores, pseudo_etiquetas = reentrenar_modelo(model_S4, (X_train_S4, y_train_S4), (X_test_S4,y_test_S4), (X_se_S4, y_se_S4), umbral=umbral)[1:]\n",
    "    scores_umbral.append(scores)\n",
    "    pseudo_etiquetas_umbral.append(pseudo_etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4, figsize=(10,7))\n",
    "plt.subplot(2,1,1)\n",
    "for umbral, scores in zip(umbrales, scores_umbral):\n",
    "    plt.plot(range(len(scores)), scores, label = f'{umbral:.2f}')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('f1 score')\n",
    "plt.title('Performance autoentrenamiento modelo S4')\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for umbral, pseudo_etiquetas in zip(umbrales,pseudo_etiquetas_umbral):\n",
    "    pseudo_etiquetas_total = [sum(pseudo_etiquetas[:i]) for i in range(1,len(pseudo_etiquetas)+1)]\n",
    "    plt.plot(range(len(pseudo_etiquetas_total)), pseudo_etiquetas_total, label = f'{umbral:.2f}')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Total etiquetados')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de las tablas de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_score(modelo, base, variables):\n",
    "    base_output = base.copy()\n",
    "    X = base_output[variables]\n",
    "    #calcular las proba de las predicciones\n",
    "    preds = modelo.predict_proba(X) #### Solo se califica la base sin smote\n",
    "    preds = np.array([ preds[i][0] for i in range(len(preds))])\n",
    "    base_output['score1'] = np.around(preds*1000, decimals = 0)\n",
    "    return base_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numero de bandas\n",
    "N=5\n",
    "\n",
    "#cargar bases para calificar y generar las tablas de validacion\n",
    "base_S1 = pd.read_csv('Bases_seg_Trainning/Base_Calificar_S1.csv')\n",
    "base_S2 = pd.read_csv('Bases_seg_Trainning/Base_Calificar_S2.csv')\n",
    "base_S3 = pd.read_csv('Bases_seg_Trainning/Base_Calificar_S3.csv')\n",
    "base_S4 = pd.read_csv('Bases_seg_Trainning/Base_Calificar_S4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model_S1 = reentrenar_modelo(model_S1, (X_train_S1, y_train_S1), (X_test_S1,y_test_S1), (X_se_S1, y_se_S1), umbral=0.8, n_iter=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_S1 = tablas_de_entrenamiento_y_validacion(obtener_score(model_S1, base_S1, variables['S1']), N,  string_score = 'score1', string_bgi = 'BGI')\n",
    "print('train')\n",
    "display(tv_S1[0])\n",
    "print('test')\n",
    "display(tv_S1[1])\n",
    "print('Calificacion del modelo')\n",
    "cal_modelo_S1 = calificar_modelo(tv_S1[0], tv_S1[1], dist_malos=0.057, long_bandas=93, KS_min=0.265,dif_KS=0.05,dist_odds=0.3)\n",
    "display(cal_modelo_S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_st_S1 = tablas_de_entrenamiento_y_validacion(obtener_score(st_model_S1, base_S1, variables['S1']), N,  string_score = 'score1', string_bgi = 'BGI')\n",
    "print('train')\n",
    "display(tv_st_S1[0])\n",
    "print('test')\n",
    "display(tv_st_S1[1])\n",
    "print('Calificacion del modelo')\n",
    "cal_modelo_S1 = calificar_modelo(tv_st_S1[0], tv_st_S1[1], dist_malos=0.057, long_bandas=93, KS_min=0.265,dif_KS=0.05,dist_odds=0.3)\n",
    "display(cal_modelo_S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calificar_modelo(tv_st_S1[0], tv_st_S1[1], dist_malos=0.09, long_bandas=120, KS_min=0.265,dif_KS=0.05,dist_odds=0.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?calificar_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26dc589a2e6421466be58b757f2befaa3fe97a288a564d2394bb2ba162fa08cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
